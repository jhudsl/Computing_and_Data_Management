[["index.html", "Data Management for Cancer Research About this course", " Data Management for Cancer Research 2021-10-19 About this course This course is part of a series of courses for the Informatics Technology for Cancer Research (ITCR) called the Informatics Technology for Cancer Research Education Resource. This material was created by the ITCR Training Network (ITN) which is a collaborative effort of researchers around the United States to support cancer informatics and data science training through resources, technology, and events. This initiative is funded by the following grant: National Cancer Institute (NCI) UE5 CA254170. Our courses feature tools developed by ITCR Investigators and make it easier for principal investigators, scientists, and analysts to integrate cancer informatics into their workflows. Please see our website at www.itcrtraining.org for more information. Except where otherwise indicated, the contents of this course are available for use under the Creative Commons Attribution 4.0 license. You are free to adapt and share the work, but you must give appropriate credit, provide a link to the license, and indicate if changes were made. Sample attribution: Data Management for Cancer Research by Johns Hopkins Data Science Lab (CC-BY 4.0). You can download the illustrations by clicking here. "],["introduction.html", "Chapter 1 Introduction 1.1 Motivation 1.2 Target Audience 1.3 Curriculum", " Chapter 1 Introduction 1.1 Motivation One of the key cancer informatics challenges is dealing with and managing the explosion of data from multiple sources. This course is designed to help researchers and investigators to understand the key principles of data management from an economic, privacy, security, usability and discoverability perspective. 1.2 Target Audience This course is intended for researchers (including postdocs and students) with limited to intermediate experience with informatics research. The conceptual material will also be useful for those in management roles who are collecting data and using informatics pipelines. 1.3 Curriculum The course will cover the key underlying principles and concepts in data management. It will cover concrete discussions of the economic choices behind cloud and local data management. It will also cover data security, data privacy, IRB and data access requests, and the ethics of good data management. The course will highlight concrete workflows for managing genomic, imaging, and clinical data. "],["computing-basics.html", "Chapter 2 Computing basics 2.1 Computing Components", " Chapter 2 Computing basics In this chapter we will describe the basics about computing, what cloud computing actually is, and methods to perform informatics work that might require more intensive computing. First we would like to start off with some background information about how computers actually work. Understanding how computers actually work will be very helpful for understanding what computing resources your research will actually require. 2.1 Computing Components Luckily, you are likely not going to need to become a bee keeper to perform your computational research (unless of course that interests you)! Instead, computers rely on millions to billions of transistors. 2.1.1 Transistors Transistors are one of if not the most important basic building blocks of computers. There are many different types of transistors, but they often look like a rectangle with three prongs. Transistors behave like electronic switches or gates that either allow or do not allow current to flow through a particular part of a circuit. [Source] Inside the plastic, is often silicon, or some other semiconductive crystal. Semiconductors are needed because they can conduct electricity (especially when combined with a layer of conductive metal) but are neutral on their own, making them the perfect option for creating an electrical switch. Silicon is especially useful, because it doesn’t cause the circuit to get very hot, unlike previously used materials. It is also very abundant, in fact, it is the second most common element of the Earth’s crust! (“Silicon - Wikipedia” n.d.)). If the transistor receives a small amount of current to one of the prongs (called the base), this turns it on, and allows the larger current for the circuit to pass through the transistor (from a prong called the collector to the prong called the emitter). If the base prong of the transistor does not receive a small current than the transistor is off and the current for the circuit is not allowed to flow through the transistor. These two states for the flow of current ultimately allow for the storage and use of binary data, which we think of a zeros and ones, but it is really the absence or presence of current with a voltage beyond a threshold for this part of the circuit. Thus the physical components of a computer are ultimately based on the assessment of only two states of current (0 (or FALSE) = below a threshold and 1 (or TRUE) = above a threshold), which is much easier to create than if it we needed to assess more nuanced levels of current. It turns out that this is all that is needed for computers to perform all the complex tasks that we rely on them for everyday. Very importantly transistors have gotten much smaller over time. The smaller size of transistors has allowed for many more transistors to be used inside computers. [Source] Both the smaller size of the transistors and the increased number of transistors within computers have in part allowed computers to become faster and more powerful (Pokropivny et al. 2007). These silicon transistors became so important for the field of electronics, that the late 20th century and early 21st century is sometimes called the “Silicon Age”. This is also why many places in the world where there are many technological institutes are often called a name with silicon, such as Silicon Valley. Here is an interesting article about what our next age might be about, and it has to do with changing the way we harness electrons (the current role of transistors) - that’s how important they are! If you would like to learn more about the history of transistors and how they work check out this website. 2.1.2 ALU - Arithmetic Logic Unit The ALU is responsible for performing simple operations by using networks (in this case commonly called a circuit) of transistors. These simple operations include logical operations (such as AND, OR, or NOT etc.), and arithmetic operations (such as addition, subtraction, or multiplication etc.). Ultimately most of what we do everyday on our computers come down to these simple operations. These operations are based on what is called Boolean logic or Boolean algebra, which was invented by George Boole in 1854 and largely comes down to thinking of possible sets of data (“How Do Logic Gates Work?” n.d.). For example, if we have two transistors, they could both be on, they could both be off, or one or the other could be on. Considering these possibilities, we can make overall descriptions about the flow of current to perform logical operations. Let’s take a moment to understand how networks of transistors work for AND and OR operations. We call a network for a logical operation a logic gate. The transistor AND gate has two transistors in series, meaning they are sequentially placed one after the other, where one receives current first before the other. A resulting high current output only occurs when both of the transistors allow for the flow of current. If either or both of the transistors is off than the current is not allowed to flow through. The transistor OR gate has two transistors in parallel, meaning they are next to one another each receiving the flow of current at the same time. A resulting high current output can occur when either of the transistors allows for the flow of current. Importantly using more complex arrangements of these logic gates can allow the computer to perform the arithmetic operations. See here and here for more information on how this works. If you would like to learn more about these gates with circuit diagrams, check out this [website] (http://hyperphysics.phy-astr.gsu.edu/hbase/Electronic/trangate.html#c1) and this website for some visualizations. This website and this website also go into great detail. In case you are wondering about the semantics of phrases like the “flow of current”, check this discussion. 2.1.3 Binary or Boolean data An ALU performs arithmetic operations using values are represented in binary digits called bits (0 or 1) (recall that this is based on a state of current). Data like this is also called Boolean, because George Boole invented a system of algebra for such data in 1854 . These values do not mean their typical meanings from what we know numerically, but instead follow arithmetic rules using 2 as the base, as opposed to 10 which we are familiar with for our decimal system. What does this mean? With our decimal system when we reach a value of 10, we start to carry over the 1. With the binary system when we reach a value of 2, we start to carry over the 1. Here we can see how the first 9 digits of the decimal system are represented in the binary system. See here to learn more about binary calculations. 2.1.4 Flip-flops and registers Flip-flops are used for storing one bit of digital binary data. They are made of transistors (that’s right it’s transistors again!) in a configuration that allows for the flip-flop to hold one of two states, thus enabling the storage of binary data. A group of flip-flops is called a register. You may have heard about a computer having a 64- or 32- bit operating system (more on what this soon). These computers have registers with 64 bits or 32 bits respectively. Thus there are 64 flip-flops within the registers of a 64-bit system. Each of these are capable of storing and processing binary values 64 digits in length (which works out to an unsigned integer in our decimal system of up to 2^64-1, or 18,446,744,073,709,551,615)! You may also be wondering how letters and other symbols are stored in this binary system. Letters are each assigned a numeric decimal value according to an encoding system such as the ASCII system, and these are converted into the binary form of the number. In the ASCII system, this ultimately works out to letters being stored by 8 binary bits or digits also called a byte (“What Is ASCII (American Standard Code for Information Interexchange)?” n.d.). Since this is consistent, this works well with systems that have registers that can store in sets of 8 bits. Since 64 divided by 8 is 8, this means for a 64-bit operating system, each register can store 8 letters or symbols at a time. Below you can see the decimal value for some of the symbols and letters: Note that ASCII has largely been replaced in 1991 for Unicode, which allows for more characters, supporting languages like Chinese that require for more characters than the 256 that ASCII could support, however this works in a similar way. 2.1.5 CPU - Central Processing Unit or Core The CPU is often called the brain of the computer. This is what people are referring to when they describe a “computer chip”. It performs and orchestrates computational tasks. The CPU is made up of several components, a few that are particularly important (two of which we have discussed): 1) ALU 2) Registers 3) Control Unit (CU) The Control Unit coordinates the ALU and the data stored in the registers, so that the ALU can perform the operations on the right data stored in the registers at the right time. Modern computers now have multiple cores. What does this mean? This means that there are multiple CPUs within the same computer chip. A dual core CPU is a chip with two CPUs. A quad-core CPU is a chip with 4 CPUs and so on.This allows modern computers to perform multiple tasks at the same time instead of sequentially, such as 4 tasks simultaneously on a current typical laptop (with 4 cores). This makes our computers much faster than they used to be. In addition to the main CPU, computers may be equipped with specialized processors called GPUs which stands for graphics processing units that are especially efficient at tasks involving images. Thus any tasks that require the involvement of images are done using the GPU and not the CPU. This frees up the CPU to continue on the tasks not involving images more efficiently. Hyper-threading is also an option for improving processing. This technology started in 2002 by Intel. The idea is that while part of the same CPU is idle or waiting for a given task, another part of the CPU can work to perform another task. This isn’t as efficient as a having another core or CPU, but it does improve efficiency. So many modern computer chips actually use all three efficiency boosters (having multiple cores, having GPUs, and using hyper-threading). Thus a chip with 4 cores that also has hyper-threading can work on 8 tasks simultaneously. Since it is now much easier to produce chips with multiple cores and because there are some security concerns with hyper-threading, the field seems to be moving away from hyper-threading. 2.1.6 Memory or RAM - short-term memory OK, so we have already talked about how data can be stored in the registers within the CPU. This data or memory is used directly by the CPU during operations or tasks. However, our CPUs need additional quick access to data to tell the CPU what to do to perform the operations. This bring us to RAM. RAM stands for Random Access Memory. It is often simply referred to as memory. Since CPUs are fast, RAM needs to be fast, making it relatively expensive. One distinctive feature of this type of memory is that it is temporary. When your computer no longer has power, the data stored in RAM disappears. This type of memory is also called volatile. is Ram just registers located outside the CPU? For more information about how RAM works, check out this website: https://computer.howstuffworks.com/ram.htm. 2.1.7 Storage - long-term memory Avocado start motivating discussing data transfers here a bit… We can also store data that we aren’t directly using when our computer is performing operations. This type of memory is called storage and is sometimes referred to as long-term or non-volatile memory because electricity is not required to preserve this data. This type of memory is stored using hard disk drives (HDD) also called hard drives or more recently solid-state drives (SSD). Typically ranging from gigabytes to terabytes or more, this type of storage offers big data capacity for a relatively low price at the cost of speed. Hard disk drives store memory using magnetic methods, while solid-state drives store memory using chips that have guess what?? They are made of yet again the important basic building block of computers - tiny bees! Oops, I mean transistors yet again, just like the CPU chip! See those transistors are really important. SSDs allow for much faster reading and writing of files, as well as increased reliability. However, they are more expensive and they also wear out eventually. https://computer.howstuffworks.com/solid-state-drive.htm Go into this??? ROM stands for Read-Only Memory. This means that the data remains statically stored and can only be rewritten in limited circumstances. https://arstechnica.com/information-technology/2012/06/inside-the-ssd-revolution-how-solid-state-disks-really-work https://www.extremetech.com/computing/88078-how-a-hard-drive-works 2.1.8 Hardware and Software So far we have talked about the hardware of a computer, while software is the code that tells the hardware how to function. Software can also be important to know about. Most importantly it is useful to know about operating systems. 2.1.9 Operating Systems The operating system (sometimes simply called the OS) is a set of code or software that translates user interactions with the computer to tell the hardware (including memory and the CPU) of the computer what tasks to do and when. You can think of this as the basic code to keep the computer running and functional and to allow the user to use other forms of software, such as applications. Applications are specialized software programs like Microsoft Word, or an internet browser like Chrome that allow a user to do specific tasks on the computer. So your OS is what allows you to name, rename, move and save files. It helps you to keep track of memory and decides what memory should be used when and to run all of your application software . It also allows you to talk to other devices like printers or other computers. Examples of commonly used operating systems are: * Microsoft Windows (such as Windows 10, Windows 11 etc.) * macOS (notice the OS here - it might make more sense now why it is called this) * Linux * Android Previously, back when a university might have one single computer, as they were so large and expensive (they didn’t use those nifty small transistors of today), computers didn’t have operating systems and only one task could be performed at a time by one person at a time. Back then, tasks were just manually started and prioritized and scheduled by humans. Tasks or programs (including sometimes data) could be printed or punched on cards (called punched cards) that would be loaded into the machine. It could really be a pain for users if they accidentally dropped the cards for the program they wanted to run, as you can imagine! See here for more information: https://www.deskdecode.com/operating-systems-os/ https://timeline.com/women-pioneered-computer-programming-then-men-took-their-industry-over-c2959b822523 https://spectrum.ieee.org/untold-history-of-ai-invisible-woman-programmed-americas-first-electronic-computer Networks, cloud computing, data transfer, servers….operating systems References "],["computing-systems.html", "Chapter 3 Computing Systems 3.1 Computing Platforms 3.2 Hardware", " Chapter 3 Computing Systems In this chapter we will provide examples of data management systems that you might find useful for your research. Please note that we aim to provide a general overview of options and thus this is not a complete list. Let us know if there is a platform or system that you think we should include! This chapter aims to provide research leaders with some guidance about making decisions for computing hardware (the physical parts of your computing platform) and software (the code that tells the computing platform how to function). It will also describe the benefits and drawbacks of local and “Cloud” computing. 3.0.1 Current Computer Capacity So how many tasks can the CPU of an average computer do these days? How much memory and storage do they typically have? These values will probably change very soon, but currently: Laptops can perform 8 CPU tasks at once, storing 64 GB in memory and 8 TB on storage. Handheld tablets can now perform 8 CPU tasks, and store 6 GB in memory and 1 TB on storage. Some phones can compete with laptops from the not so distant past by performing 6 CPU tasks at once and storing 4 GB in memory and 0.5 TB on storage. 3.0.2 Servers What if we need to more computational power than our laptop? You may encounter times where certain informatics tasks take way too long or are not even possible on your personal computer. In terms of hardware, the term server means a computer or computers that can be accessed through a direct local network or the internet to perform computations or store data. Read here to learn more. For example, your lab members could connect to a server from their own computers to allow each of them more computational power. Typically computers that act as servers are set up a bit differently than our personal computers, as they do not need the same functionality. For instance they often don’t have capabilities to support a graphical interface (more on what that is later). They are designed to optimize data storage and computational power. 3.1 Computing Platforms Now that we have discussed a bit about how computers perform computations, lets discuss more about how you might choose your computing platform. A computing platform, is all the hardware (the physical parts of your computing platform) and software (the code that tells the computing platform how to function) necessary to create the environment in which you can perform your computational work. Choosing a computing platform involves both software and hardware decisions. We will focus on hardware. 3.2 Hardware With regards to hardware, there are two major options: Personal computers Shared computers 3.2.1 Personal computers These are computers that your lab might own, such as a laptop, a desktop, or a small server used by just your lab. If you are not performing intensive computational tasks, it is possible that you will only need personal computers for your lab. However, you may find that this changes, and you might require connecting your personal computers to shared computers for more computational power and or storage. Note that you could technically purchase your own server. However, this is very likely not as economical, feasible (as having your own server would require that you personally maintain it), or salable (you might outgrow your server) as other computing options that we will discuss in a bit. https://profs.info.uaic.ro/~adria/teach/courses/pcdfeaa/resources/C5_PCD_FEEA_ClusterGridComputing_en.pdf 3.2.2 Shared Computers These are servers (groups of computers) that are shared with other people that you can connect to from your computer (typically using the internet) to help you perform more intensive computational tasks or to store large amounts of data. Among shared computers there are three major options: Clusters - institutional or national resources Grids - institutional or national resources “Cloud” - commercial or national resources AVOCADO - maybe put the image here for shared computers - check how anvil images were added within lists Computer Cluster In a computing cluster several of the same type of computer (often in close proximity and connected by a local area network rather than the internet) work together to perform pieces of the same single task simultaneously. The idea of performing multiple computations simultaneously is called parallel computing. There are different designs or architectures for clusters. One common one is the Beowulf cluster in which a master computer (called front node or server node) breaks a task up into small pieces that the other computers (called client nodes or simply nodes) perform. For example, if a large file needs to be converted to a different format, pieces of the file will be converted simultaneously by the different nodes. Thus each node is performing the same task just with different pieces of the file. The user has to write code in a special way to specify that they want parallel processing to be used and how. It is important to realize that the CPUs in each of the node computers connected within a cluster are all performing a similar task simultaneously. See here for more information. Computer Grid In a computing grid different types of computers (often in different locations) work towards an overall common goal by performing different tasks. The concept for grid computing is similar to that of an electric power grid, where only computers (nodes) actively performing a task are using resources at any given time. Again, just like computer clusters, there are many types of architectures that can be rather simple to very complex. For example you can think of different universities collaborating to perform different computations for the same project. One university might perform computations using gene expression data about a particular population, while another performs computations using data from another population. Importantly each of these universities might use clusters to perform their specific task. Both grids and clusters use a special type of software called middleware to coordinate the various computers involved. Users need to write their scripts in a way that can be performed by multiple computers simultaneously. Users also need to be conscious of how to schedule their tasks and to follow the rules and etiquette of the specific cluster or grid that they are sharing. See here for more information about the difference between clusters and grids. AVOCADO maybe add as a reference https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&amp;arnumber=1300502 “Cloud” computing] https://go.cloudhealthtech.com/eb-simplify-the-journey-data-center-to-public-cloud.html https://www.geeksforgeeks.org/difference-between-grid-computing-and-cluster-computing/ More recently, commercial “Cloud” solutions are becoming a more viable hardware solution, offered by companies like Amazon, Google, and Microsoft. This option is technically also a shared computer situation. Somewhere these companies have clusters of computers that paying customers use through the internet. In addition there are options like Jetstream which is a more “Cloud-like” part of Xsede. AVOCADO need a new shared computer image and more about Jetstream which I think is a good option for many people 3.2.3 Accessing Shared Computer Resources All of the shared computing options that already exist and that you could utilize involve a data center where are large number of computers are physically housed. Photo by Taylor Vick on Unsplash You may have access to a HPC (which stands for High Performance Computing) cluster at your institute. Or you could consider using national resources like Xsede. Your university or institution may have a HPC cluster, this means that they have a group of computers acting like servers that people can use to store data or assist with intensive computations. Often institutions can support the cost of many computers within an HPC cluster, allowing for what is called parallel computing. This means that multiple computers will simultaneously perform different parts of the computing required for a given task, thus significantly speeding up the process compared to you trying to perform the task on just your computer! This is also a much more cost effective option than having one expensive supercomputer (a computer that individually has the computational power of many personal computers) to act as a server. It turns out that buying several less powerful computers is cheaper. AVOCADO - update image below to say cluster and grid access Alternatively, you could consider an option like Xsede. Xsede is led by the University of Illinois National Center for Supercomputing Applications (NCSA) and includes 18 other partnering institutions (which are mostly other universities). Through this partnership, they currently support 16 supercomputers. Universities and non-profit researchers in the United States can request access to their computational and data storage resources. Here you can see a photo of Stampede2, one of the supercomputers that members of Xsede can utilize. [source] Stampede2, generously funded by the National Science Foundation (NSF) through award ACI-1134872, is one of the Texas Advanced Computing Center (TACC), University of Texas at Austin’s flagship supercomputers. See here for more information about how you could possibly connect to and utilize Stampede2. Importantly when you use shared computers like national resources like Stampede2 available through Xsede, as well as institutional HPC clusters, you will share these resources with many other people and so you need to learn the proper etiquette for using and sharing these resources. These will vary by the shared resource, however in general: Don’t use all nodes if you don’t need to Don’t use all RAM on a node if you don’t need to Communicate with others if you will be submitting a large or intensive job 3.2.4 Cloud Computing https://go.cloudhealthtech.com/eb-simplify-the-journey-data-center-to-public-cloud.html https://www.geeksforgeeks.org/difference-between-grid-computing-and-cluster-computing/ More recently, commercial “Cloud” solutions are becoming a more viable hardware solution, offered by companies like Amazon, Google, and Microsoft. This option is technically also a shared computer situation. Somewhere these companies have clusters of computers that paying customers use through the internet. In addition there are options like Jetstream which is a more “Cloud-like” part of Xsede. What is the difference between the “Cloud” and other shared computer options? 3.2.5 Galaxy This section was written by Jeremy Goecks: Galaxy is a web-based computational workbench that connects analysis tools, biomedical datasets, computing resources, a graphical user interface, and a programmatic API (Figure 1). Galaxy (https://galaxyproject.org/) enables accessible, reproducible, and collaborative biomedical data science by anyone regardless of their informatics expertise. There are more than 8,000 analysis tools and 200 visualizations integrated into Galaxy that can be used to process a wide variety of biomedical datasets. This includes tools for analyzing genomic, transcriptomic (RNA-seq), proteomic, metabolomic, microbiome, and imaging datasets, tool suites for single-cell omics and machine learning, and thousands of more tools. Galaxy’s graphical user interface can be used with only a web browser, and there is a programmatic API for performing scripted and automated analyses with Galaxy. Galaxy is used daily by thousands of scientists across the world. A vibrant Galaxy community has deployed hundreds of Galaxy servers across the world, including more than 150 public and three large national/international servers in the United States, Europe, and Australia (https://usegalaxy.org, https://usegalaxy.eu, https://usegalaxy.org.au). The three national/international servers (Figure 2) have more than 250,000 registered users who execute &gt;500,000 analysis jobs each month. Galaxy has been cited more than 10,000 times with &gt;20% from papers related to cancer. The Galaxy Tool Shed (https://usegalaxy.org/toolshed) provides a central location where developers can upload tools and visualizations and users can search and install tools and visualizations into any Galaxy server. Galaxy has a large presence in the cancer research community. Galaxy serves as an integration and/or analysis platform for 7 projects in the NCI ITCR program. There is also increasing use of Galaxy in key NIH initiatives such as the NCI Cancer Moonshot Human Tumor Atlas Network (HTAN) and the NHGRI Data Commons, called the AnVIL (https://anvilproject.org/). Galaxy’s user interface, accessible via a web browser, provides access to all Galaxy functionality. The main Galaxy interface (Figure 3) has three panels: available tools (left), running analyses and viewing data (middle), and a full history of tools run and datasets generated (right). Datasets for analysis in Galaxy can be uploaded from a laptop or desktop computer or obtained from public data repositories connected to Galaxy. With Galaxy, complex workflows composed of tens or even hundreds of analysis tools can be created and run. In Galaxy’s workflow interface (Figure 4), tools can be added and connected via a simple drag-and-drop approach. Galaxy users can share all their work—analysis histories, workflows, and visualizations—via simple URLs that are available to specific colleagues or a link that anyone can access. Galaxy’s user interface is highly scalable. Tens, hundreds, or even thousands of datasets can be grouped into collections and run in parallel using individual tools or multi-tool workflows. In summary, Galaxy is a popular computational workbench with tools and features for a wide variety of data analyses, and it has broad usage in cancer data analysis. "],["data-management-platforms-and-systems.html", "Chapter 4 Data Management Platforms and Systems", " Chapter 4 Data Management Platforms and Systems In this chapter we will provide examples of data management systems that you might find useful for your research. Please note that we aim to provide a general overview of options and thus this is not a complete list. Let us know if there is a platform or system that you think we should include! 4.0.1 Galaxy This section was written by Jeremy Goecks: Galaxy is a web-based computational workbench that connects analysis tools, biomedical datasets, computing resources, a graphical user interface, and a programmatic API (Figure 1). Galaxy (https://galaxyproject.org/) enables accessible, reproducible, and collaborative biomedical data science by anyone regardless of their informatics expertise. There are more than 8,000 analysis tools and 200 visualizations integrated into Galaxy that can be used to process a wide variety of biomedical datasets. This includes tools for analyzing genomic, transcriptomic (RNA-seq), proteomic, metabolomic, microbiome, and imaging datasets, tool suites for single-cell omics and machine learning, and thousands of more tools. Galaxy’s graphical user interface can be used with only a web browser, and there is a programmatic API for performing scripted and automated analyses with Galaxy. Galaxy is used daily by thousands of scientists across the world. A vibrant Galaxy community has deployed hundreds of Galaxy servers across the world, including more than 150 public and three large national/international servers in the United States, Europe, and Australia (https://usegalaxy.org, https://usegalaxy.eu, https://usegalaxy.org.au). The three national/international servers (Figure 2) have more than 250,000 registered users who execute &gt;500,000 analysis jobs each month. Galaxy has been cited more than 10,000 times with &gt;20% from papers related to cancer. The Galaxy Tool Shed (https://usegalaxy.org/toolshed) provides a central location where developers can upload tools and visualizations and users can search and install tools and visualizations into any Galaxy server. Galaxy has a large presence in the cancer research community. Galaxy serves as an integration and/or analysis platform for 7 projects in the NCI ITCR program. There is also increasing use of Galaxy in key NIH initiatives such as the NCI Cancer Moonshot Human Tumor Atlas Network (HTAN) and the NHGRI Data Commons, called the AnVIL (https://anvilproject.org/). Galaxy’s user interface, accessible via a web browser, provides access to all Galaxy functionality. The main Galaxy interface (Figure 3) has three panels: available tools (left), running analyses and viewing data (middle), and a full history of tools run and datasets generated (right). Datasets for analysis in Galaxy can be uploaded from a laptop or desktop computer or obtained from public data repositories connected to Galaxy. With Galaxy, complex workflows composed of tens or even hundreds of analysis tools can be created and run. In Galaxy’s workflow interface (Figure 4), tools can be added and connected via a simple drag-and-drop approach. Galaxy users can share all their work—analysis histories, workflows, and visualizations—via simple URLs that are available to specific colleagues or a link that anyone can access. Galaxy’s user interface is highly scalable. Tens, hundreds, or even thousands of datasets can be grouped into collections and run in parallel using individual tools or multi-tool workflows. In summary, Galaxy is a popular computational workbench with tools and features for a wide variety of data analyses, and it has broad usage in cancer data analysis. "],["data-management-decisions.html", "Chapter 5 Data Management Decisions 5.1 Computing Platforms 5.2 Hardware 5.3 Choosing a Computing Platform 5.4 Local Costs 5.5 Cloud Costs", " Chapter 5 Data Management Decisions In this chapter we will discuss aspects that you should consider when deciding what data and computing management systems to use for your own work. To afford you the best opportunity to perform the informatics research that you would like, it is useful to become familiar with computing options and costs. This course aims to provide research leaders with some guidance about making decisions for computing hardware (the physical parts of your computing platform) and software (the code that tells the computing platform how to function). It will also describe the benefits and drawbacks of local and “Cloud” computing, as well as the associated costs of each. Note: This content was adapted from content by Frederick Tan for the AnVIL project. See his course created with Jeff Leek, Sarah Wheelan, and Kai Kammers here. 5.1 Computing Platforms Now that we have discussed a bit about how computers perform computations, lets discuss more about how you might choose your computing platform. A computing platform, is all the hardware (the physical parts of your computing platform) and software (the code that tells the computing platform how to function) necessary to create the environment in which you can perform your computational work. Choosing a computing platform involves both software and hardware decisions. We will focus on hardware. 5.2 Hardware With regards to hardware, there are two major options: Personal computers Shared computers 5.2.1 Personal computers These are computers that your lab might own, such as a laptop, a desktop, or a small server used by just your lab. If you are not performing intensive computational tasks, it is possible that you will only need personal computers for your lab. However, you may find that this changes, and you might require connecting your personal computers to shared computers for more computational power and or storage. Note that you could technically purchase your own server. However, this is very likely not as economical, feasible (as having your own server would require that you personally maintain it), or scalable (you might outgrow your server) as other computing options that we will discuss in a bit. https://profs.info.uaic.ro/~adria/teach/courses/pcdfeaa/resources/C5_PCD_FEEA_ClusterGridComputing_en.pdf 5.2.2 Shared Computers These are servers (groups of computers) that are shared with other people that you can connect to from your computer (typically using the internet) to help you perform more intensive computational tasks or to store large amounts of data. Among shared computers there are three major options: Clusters - institutional or national resources Grids - institutional or national resources “Cloud” - commercial or national resources AVOCADO - maybe put the image here for shared computers - check how anvil images were added within lists Computer Cluster In a computing cluster several of the same type of computer (often in close proximity and connected by a local area network rather than the internet) work together to perform pieces of the same single task simultaneously. The idea of performing multiple computations simultaneously is called parallel computing. There are different designs or architectures for clusters. One common one is the Beowulf cluster in which a master computer (called front node or server node) breaks a task up into small pieces that the other computers (called client nodes or simply nodes) perform. For example, if a large file needs to be converted to a different format, pieces of the file will be converted simultaneously by the different nodes. Thus each node is performing the same task just with different pieces of the file. The user has to write code in a special way to specify that they want parallel processing to be used and how. It is important to realize that the CPUs in each of the node computers connected within a cluster are all performing a similar task simultaneously. See here for more information. Computer Grid In a computing grid different types of computers (often in different locations) work towards an overall common goal by performing different tasks. The concept for grid computing is similar to that of an electric power grid, where only computers (nodes) actively performing a task are using resources at any given time. Again, just like computer clusters, there are many types of architectures that can be rather simple to very complex. For example you can think of different universities collaborating to perform different computations for the same project. One university might perform computations using gene expression data about a particular population, while another performs computations using data from another population. Importantly each of these universities might use clusters to perform their specific task. Both grids and clusters use a special type of software called middleware to coordinate the various computers involved. Users need to write their scripts in a way that can be performed by multiple computers simultaneously. Users also need to be conscious of how to schedule their tasks and to follow the rules and etiquette of the specific cluster or grid that they are sharing. See here for more information about the difference between clusters and grids. AVOCADO maybe add as a reference https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&amp;arnumber=1300502 “Cloud” computing] https://go.cloudhealthtech.com/eb-simplify-the-journey-data-center-to-public-cloud.html https://www.geeksforgeeks.org/difference-between-grid-computing-and-cluster-computing/ More recently, commercial “Cloud” solutions are becoming a more viable hardware solution, offered by companies like Amazon, Google, and Microsoft. This option is technically also a shared computer situation. Somewhere these companies have clusters of computers that paying customers use through the internet. In addition there are options like Jetstream which is a more “Cloud-like” part of Xsede. AVOCADO need a new shared computer image 5.2.3 Accessing Shared Computer Resources All of the shared computing options that already exist and that you could utilize involve a data center where are large number of computers are physically housed. Photo by Taylor Vick on Unsplash You may have access to a HPC (which stands for High Performance Computing) cluster at your institute. Or you could consider using national resources like Xsede. Your university or institution may have a HPC cluster, this means that they have a group of computers acting like servers that people can use to store data or assist with intensive computations. Often institutions can support the cost of many computers within an HPC cluster, allowing for what is called parallel computing. This means that multiple computers will simultaneously perform different parts of the computing required for a given task, thus significantly speeding up the process compared to you trying to perform the task on just your computer! This is also a much more cost effective option than having one expensive supercomputer (a computer that individually has the computational power of many personal computers) to act as a server. It turns out that buying several less powerful computers is cheaper. AVOCADO - update image below to say cluster and grid access Alternatively, you could consider an option like Xsede. Xsede is led by the University of Illinois National Center for Supercomputing Applications (NCSA) and includes 18 other partnering institutions (which are mostly other universities). Through this partnership, they currently support 16 supercomputers. Universities and non-profit researchers in the United States can request access to their computational and data storage resources. Here you can see a photo of Stampede2, one of the supercomputers that members of Xsede can utilize. [source] Stampede2, generously funded by the National Science Foundation (NSF) through award ACI-1134872, is one of the Texas Advanced Computing Center (TACC), University of Texas at Austin’s flagship supercomputers. See here for more information about how you could possibly connect to and utilize Stampede2. Importantly when you use shared computers like national resources like Stampede2 available through Xsede, as well as institutional HPC clusters, you will share these resources with many other people and so you need to learn the proper etiquette for using and sharing these resources. These will vary by the shared resource, however in general: Don’t use all nodes if you don’t need to Don’t use all RAM on a node if you don’t need to Communicate with others if you will be submitting a large or intensive job 5.2.4 Cloud Computing https://go.cloudhealthtech.com/eb-simplify-the-journey-data-center-to-public-cloud.html https://www.geeksforgeeks.org/difference-between-grid-computing-and-cluster-computing/ More recently, commercial “Cloud” solutions are becoming a more viable hardware solution, offered by companies like Amazon, Google, and Microsoft. This option is technically also a shared computer situation. Somewhere these companies have clusters of computers that paying customers use through the internet. In addition there are options like Jetstream which is a more “Cloud-like” part of Xsede. What is the difference between the “Cloud” and other shared computer options? 5.3 Choosing a Computing Platform Choosing a computing platform depends on many different considerations. 5.3.1 Important questions Asking yourself and your research team these questions can help you find the right computing platform: Do I need a graphical interface, a command line interface, or both? What do we mean by this? A graphical interface or graphical user interface or GUI, allows for users to choose functions to perform by interacting with visual representations. They have a “user-centered” design that creates a visual environment where users can for example click on tabs, boxes, or icons for to perform functions. Galaxy offers a graphical interface for performing analyses and tasks. For example in the following image we show a GUI for joining two files: A command line interface (also known as a character interface) allows for software functions to be performed by specifying through commands written in text. This typically offers more control than a graphical interface, however command line interfaces are often less user friendly as they require that the user know the correct commands to use. For example, one could perform functions in R using Bioconductor packages such as Biostrings with a command line interface: A situation where you might use both a command line interface and a GUI, is using RStudio to perform an analysis in R with Bioconductor packages. RStudio is what is called an IDE or an integrated development environment, which is an application that supports writing code. There are many tools to help you including a console for writing code in R with command line interfacing, as well as graphical interface tools. As shown in this example below, one can inspect and save a plot (that was created with the command line) by using a GUI. 2) Am I working with protected data that requires special security precautions? If you are working with data that might be protected by HIPAA, such as electronic health records, then special security measures are required to ensure that only authorized users have access to the data. [source] How computationally intensive are my tasks? If you have a large amount of data and/or are performing complex analyses, you may require more computational power than your current laptop can provide. If this is the case, you might consider using a local server or what is called “Cloud” computing (more on that later). How much storage space do I need for both temporary and long-term data? If you are working with large datasets you may also need storage options that go beyond what you currently have available. Local or “Cloud” storage options may work for you, depending on other considerations (security, data transfers) that we will discuss further. Avocado - I want to modify this to highlight the difference between cloud computing by companies vs Xsede Are my local resources sufficient? When a local solution already works, one may rightly question the time required to migrate to the Cloud. However, when local solutions are insufficient or unsustainable, then the Cloud becomes a competitive option. Am I working with especially big or controlled access datasets? Increasingly large datasets like the NCBI Sequence Read Archive are being stored on the Cloud. If your work relies on being able to access the entire dataset, then the Cloud may be your only practical option. Furthermore, if you work with controlled access data, then more platforms are providing compliance with regulations like HIPAA and FedRAMP. Do I need to work with collaborators? Computational research increasingly involves larger and larger collaborations. While many fragmented systems exist to share work, the Cloud presents an opportunity for everyone to share the exact same computational environment including hardware, software, and datasets. If Cloud Computing makes sense for you, then you’re in luck! The past decade has seen the development of many efforts to make Cloud computing, easier, faster, and more affordable. As each platform has their strengths and weaknesses, we will now discuss several opportunities and challenges that Cloud computing presents in the field of computational genomics. 5.3.2 Benefits of Cloud Computing The state of Cloud computing is continually evolving. Here, we highlight three main current benefits: https://jetstream-cloud.org/files/Jetstream-Outreach-C2Exchange-Sep2019.pdf Sharing Workflows The first major benefit is the increasing ease with which one can share and collaborate on research projects. Shown here is the History feature of Galaxy. Using this, one can share not only what datasets they used but also every computational manipulation that was performed. By sharing such a history, one can reproduce an analysis in its entirety, allowing collaborators to offer comments and extend upon the work with ease. Sharing Workflows between Platforms While sharing complete analysis histories is for the most part constrained to a particular software platform, a second benefit that has arisen is the ability to share Workflows between platforms. Shown here is a diagram of a single cell analysis pipeline published by the Klarman Cell Observatory on Dockstore: This higher level abstraction coupled with container technology allows this multistep analysis to be run with relative ease on supporting platforms like Terra or DNAnexus. Using Commodity Hardware The third Benefit we highlight is the increasing ease by which one can provision commodity hardware at scale. What this means is that you can pay reasonable costs to complete your analysis in less time by renting hundreds to tens of thousands of Cloud-based computers – importantly stopping the bill when your analysis is complete. Specialized hardware like GPUs and large memory nodes are also available for rent allowing you to pay only for what you need. 5.3.3 Challenges of Cloud Computing Balancing these three benefits are four challenges: Data Transfer Data transfer and data management remains a cumbersome task. While storing data in the “Cloud” has its advantages, it also has corresponding storage costs. Thus, careful planning is necessary with regards to what data will be stored where, as well as budgeting the time necessary to transfer data back and forth. Data Security Most Cloud resources offer features that make it easier to access and share data, and these features often come at the expense of security. Thus, special precautions must be implemented to securely store protected datasets such as human genome sequences and electronic health records. Costs Controlling costs, especially with regards to storage, presents a third formidable challenge. As many Cloud providers naturally want to encourage usage of their platforms, users must be aware of how much money is currently being spent and be able to project how much money is likely to be spent in the future. We will briefly overview cost controls in the next section. While software platforms can help mitigate these challenges, Cloud computing still incurs costs from the underlying hardware providers. IT A final challenge is that many IT support staff do not have extensive experience managing Cloud resources. Should IT choose to support analysis on the Cloud, they would face the aforementioned challenges of understanding and supporting data management, security compliance, and cost management. Fortunately, large initiatives like AnVIL, Galaxy, and CyVerse continue to work on democratizing access to Cloud computing by tackling many of these challenges. 5.4 Local Costs 5.5 Cloud Costs "],["data-security.html", "Chapter 6 Data Security", " Chapter 6 Data Security In this chapter we will discuss best practices for keeping your data safe and secure. "],["data-privacy.html", "Chapter 7 Data Privacy", " Chapter 7 Data Privacy Cancer research often involves personal health data that requires compliance with Health Insurance Portability and Accountability Act (HIPAA) regulations. In this chapter we will discuss data management strategies to maintain compliance with these important regulations. "],["data-sharing.html", "Chapter 8 Data Sharing 8.1 Data sharing is important!", " Chapter 8 Data Sharing In this chapter we will discuss the importance of data sharing, best practices for data sharing, as well as methods to share data in contexts in which you might have thought sharing was not possible! 8.1 Data sharing is important! Sharing data is critical for optimizing the advancement of scientific understanding. Now that labs all over the world are producing massive amounts of data, there are many discoveries that can be made by just using this existing data. This is so important, that starting in January, 2023 the NIH will require specific sharing practices for data management and sharing. See the announcement here. "],["data-ethics.html", "Chapter 9 Data Ethics", " Chapter 9 Data Ethics Now that we have covered the basics of data management, we will take a moment to consider and reflect on the implications of our use and sharing of data. "],["about-the-authors.html", "About the Authors", " About the Authors These credits are based on our course contributors table guidelines.     In memory of James Taylor, who was instrumental in initiating this project.   These are currently from another course … will update later Credits Names Pedagogy Lead Content Instructor Carrie Wright Content Editors/Reviewers Candace Savonen, Sarah Wheelan, Jeff Leek Content Directors Jeff Leek, Sarah Wheelan Content Consultants (Promoting diversity equity and inclusion) Simone Sawyer, Karriem Watson Acknowledgments Andrei Kucharavy, Sarah Opitz, Florian Markowetz, Brody Foy, Michael Mullarkey, Anne Carpenter, Luis Pedro Coelho, Keri Martinowich Production Content Publisher Ira Gooding Content Publishing Reviewers Ira Gooding, Candace Savonen Technical Course Publishing Engineer Carrie Wright Template Publishing Engineers Candace Savonen, Carrie Wright Publishing Maintenance Engineer Candace Savonen Technical Publishing Stylists Carrie Wright, Candace Savonen Package Developers (Leanbuild) John Muschelli, Candace Savonen, Carrie Wright Art and Design Illustrator Carrie Wright Funding Funder National Cancer Institute (NCI) UE5 CA254170 Funding Staff Emily Voeglein, Fallon Bachman   ## ─ Session info ─────────────────────────────────────────────────────────────── ## setting value ## version R version 4.0.2 (2020-06-22) ## os Ubuntu 20.04.2 LTS ## system x86_64, linux-gnu ## ui X11 ## language (EN) ## collate en_US.UTF-8 ## ctype en_US.UTF-8 ## tz Etc/UTC ## date 2021-10-19 ## ## ─ Packages ─────────────────────────────────────────────────────────────────── ## package * version date lib source ## assertthat 0.2.1 2019-03-21 [1] RSPM (R 4.0.3) ## backports 1.1.10 2020-09-15 [1] RSPM (R 4.0.2) ## bookdown 0.24 2021-09-29 [1] Github (rstudio/bookdown@88bc4ea) ## callr 3.4.4 2020-09-07 [1] RSPM (R 4.0.2) ## cli 2.0.2 2020-02-28 [1] RSPM (R 4.0.0) ## crayon 1.3.4 2017-09-16 [1] RSPM (R 4.0.0) ## desc 1.2.0 2018-05-01 [1] RSPM (R 4.0.3) ## devtools 2.3.2 2020-09-18 [1] RSPM (R 4.0.3) ## digest 0.6.25 2020-02-23 [1] RSPM (R 4.0.0) ## ellipsis 0.3.1 2020-05-15 [1] RSPM (R 4.0.3) ## evaluate 0.14 2019-05-28 [1] RSPM (R 4.0.3) ## fansi 0.4.1 2020-01-08 [1] RSPM (R 4.0.0) ## fs 1.5.0 2020-07-31 [1] RSPM (R 4.0.3) ## glue 1.4.2 2020-08-27 [1] RSPM (R 4.0.3) ## htmltools 0.5.0 2020-06-16 [1] RSPM (R 4.0.1) ## jquerylib 0.1.1 2020-04-30 [1] RSPM (R 4.0.0) ## knitr 1.33 2021-09-29 [1] Github (yihui/knitr@a1052d1) ## lifecycle 1.0.0 2021-02-15 [1] CRAN (R 4.0.2) ## magrittr 1.5 2014-11-22 [1] RSPM (R 4.0.0) ## memoise 1.1.0 2017-04-21 [1] RSPM (R 4.0.0) ## pkgbuild 1.1.0 2020-07-13 [1] RSPM (R 4.0.2) ## pkgload 1.1.0 2020-05-29 [1] RSPM (R 4.0.3) ## prettyunits 1.1.1 2020-01-24 [1] RSPM (R 4.0.3) ## processx 3.4.4 2020-09-03 [1] RSPM (R 4.0.2) ## ps 1.3.4 2020-08-11 [1] RSPM (R 4.0.2) ## purrr 0.3.4 2020-04-17 [1] RSPM (R 4.0.3) ## R6 2.4.1 2019-11-12 [1] RSPM (R 4.0.0) ## remotes 2.2.0 2020-07-21 [1] RSPM (R 4.0.3) ## rlang 0.4.10 2021-09-29 [1] Github (r-lib/rlang@f0c9be5) ## rmarkdown 2.10 2021-09-29 [1] Github (rstudio/rmarkdown@02d3c25) ## rprojroot 1.3-2 2018-01-03 [1] RSPM (R 4.0.0) ## sessioninfo 1.1.1 2018-11-05 [1] RSPM (R 4.0.3) ## stringi 1.5.3 2020-09-09 [1] RSPM (R 4.0.3) ## stringr 1.4.0 2019-02-10 [1] RSPM (R 4.0.3) ## testthat 3.0.1 2021-09-29 [1] Github (R-lib/testthat@e99155a) ## usethis 2.0.1.9000 2021-09-29 [1] Github (r-lib/usethis@3398055) ## withr 2.3.0 2020-09-22 [1] RSPM (R 4.0.2) ## xfun 0.26 2021-09-29 [1] Github (yihui/xfun@74c2a66) ## yaml 2.2.1 2020-02-01 [1] RSPM (R 4.0.3) ## ## [1] /usr/local/lib/R/site-library ## [2] /usr/local/lib/R/library "],["references.html", "References", " References "],["404.html", "Page not found", " Page not found The page you requested cannot be found (perhaps it was moved or renamed). You may want to try searching to find the page's new location, or use the table of contents to find the page you are looking for. "]]
